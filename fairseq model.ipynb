{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vjphHa8vhcr",
        "outputId": "452b6c85-c05b-4ad7-ab31-37e3baf3ba74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.10)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2024.5.15)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq)\n",
            "  Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.4)\n",
            "Collecting bitarray (from fairseq)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.25.2)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.12.2)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading portalocker-2.10.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.15.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->fairseq)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->fairseq)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->fairseq)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->fairseq)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->fairseq)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->fairseq)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->fairseq)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->fairseq)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->fairseq)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->fairseq)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->fairseq)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->fairseq)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11289144 sha256=8b5520ea55bc1b9904b505a038e507025e496917f57a1083908653c5473aaba2\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141211 sha256=f0c068e6a95a6d6e7503e283b03fc297b98b1707606d39508e942bc683259aa4\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, colorama, sacrebleu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, hydra-core, nvidia-cusolver-cu12, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.9.2 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 omegaconf-2.0.6 portalocker-2.10.0 sacrebleu-2.4.2\n"
          ]
        }
      ],
      "source": [
        "! pip install fairseq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KugJh3uS0Sru"
      },
      "outputs": [],
      "source": [
        "!head -n 1000 train.en > valid.en\n",
        "!tail -n +1001 train.en > train.en.new\n",
        "!mv train.en.new train.en\n",
        "\n",
        "!head -n 1000 train.gu > valid.gu\n",
        "!tail -n +1001 train.gu > train.gu.new\n",
        "!mv train.gu.new train.gu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHi5OloRxOCj",
        "outputId": "2bf6e583-1896-4fd0-df27-b748426d9201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-29 17:16:03.684382: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-29 17:16:03.684435: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-29 17:16:03.767188: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-29 17:16:03.778561: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-29 17:16:04.973567: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-06-29 17:16:07 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2024-06-29 17:16:08 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='en', target_lang='gu', trainpref='train', validpref='valid', testpref=None, align_suffix=None, destdir='data-bin/en-gu', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=1, dict_only=False)\n",
            "2024-06-29 17:16:12 | INFO | fairseq_cli.preprocess | [en] Dictionary: 17312 types\n",
            "2024-06-29 17:16:18 | INFO | fairseq_cli.preprocess | [en] train.en: 64000 sents, 728855 tokens, 0.0% replaced (by <unk>)\n",
            "2024-06-29 17:16:18 | INFO | fairseq_cli.preprocess | [en] Dictionary: 17312 types\n",
            "2024-06-29 17:16:18 | INFO | fairseq_cli.preprocess | [en] valid.en: 1000 sents, 11188 tokens, 1.58% replaced (by <unk>)\n",
            "2024-06-29 17:16:18 | INFO | fairseq_cli.preprocess | [gu] Dictionary: 25224 types\n",
            "2024-06-29 17:16:25 | INFO | fairseq_cli.preprocess | [gu] train.gu: 64000 sents, 608298 tokens, 0.0% replaced (by <unk>)\n",
            "2024-06-29 17:16:25 | INFO | fairseq_cli.preprocess | [gu] Dictionary: 25224 types\n",
            "2024-06-29 17:16:25 | INFO | fairseq_cli.preprocess | [gu] valid.gu: 1000 sents, 9527 tokens, 2.61% replaced (by <unk>)\n",
            "2024-06-29 17:16:25 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/en-gu\n"
          ]
        }
      ],
      "source": [
        "!fairseq-preprocess \\\n",
        "  --source-lang en \\\n",
        "  --target-lang gu \\\n",
        "  --trainpref train \\\n",
        "  --validpref valid \\\n",
        "  --destdir data-bin/en-gu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-wm-90OzFID",
        "outputId": "845756d4-7fd9-49b8-dba8-b67061e3e84c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-29 17:36:11.946310: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-29 17:36:11.946366: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-29 17:36:11.947814: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-29 17:36:11.956097: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-29 17:36:13.070872: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-06-29 17:36:14 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2024-06-29 17:36:15 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2024-06-29 17:36:17 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/transformer', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 5, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4096, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4096, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=100, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/transformer', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=5, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin/en-gu', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, encoder_layers=6, decoder_layers=6, encoder_attention_heads=8, decoder_attention_heads=8, encoder_embed_dim=512, decoder_embed_dim=512, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=False, share_all_embeddings=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'data-bin/en-gu', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-06-29 17:36:18 | INFO | fairseq.tasks.translation | [en] dictionary: 17312 types\n",
            "2024-06-29 17:36:18 | INFO | fairseq.tasks.translation | [gu] dictionary: 25224 types\n",
            "2024-06-29 17:36:19 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(17312, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(25224, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=25224, bias=False)\n",
            "  )\n",
            ")\n",
            "2024-06-29 17:36:19 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2024-06-29 17:36:19 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2024-06-29 17:36:19 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2024-06-29 17:36:19 | INFO | fairseq_cli.train | num. shared model params: 97,733,632 (num. trained: 97,733,632)\n",
            "2024-06-29 17:36:19 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2024-06-29 17:36:19 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: data-bin/en-gu/valid.en-gu.en\n",
            "2024-06-29 17:36:19 | INFO | fairseq.data.data_utils | loaded 1,000 examples from: data-bin/en-gu/valid.en-gu.gu\n",
            "2024-06-29 17:36:19 | INFO | fairseq.tasks.translation | data-bin/en-gu valid en-gu 1000 examples\n",
            "2024-06-29 17:36:19 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-06-29 17:36:19 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2024-06-29 17:36:19 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-06-29 17:36:19 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2024-06-29 17:36:19 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
            "2024-06-29 17:36:19 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 17:36:21 | INFO | fairseq.trainer | Loaded checkpoint checkpoints/transformer/checkpoint_last.pt (epoch 11 @ 1858 updates)\n",
            "2024-06-29 17:36:21 | INFO | fairseq.trainer | loading train data for epoch 11\n",
            "2024-06-29 17:36:21 | INFO | fairseq.data.data_utils | loaded 64,000 examples from: data-bin/en-gu/train.en-gu.en\n",
            "2024-06-29 17:36:21 | INFO | fairseq.data.data_utils | loaded 64,000 examples from: data-bin/en-gu/train.en-gu.gu\n",
            "2024-06-29 17:36:21 | INFO | fairseq.tasks.translation | data-bin/en-gu train en-gu 64000 examples\n",
            "2024-06-29 17:36:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 011:   0% 0/186 [00:00<?, ?it/s]2024-06-29 17:36:21 | INFO | fairseq.trainer | begin training epoch 11\n",
            "2024-06-29 17:36:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "epoch 011:   2% 3/186 [00:01<01:26,  2.12it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "epoch 011:  99% 185/186 [00:30<00:00,  7.01it/s, loss=5.423, nll_loss=4.026, ppl=16.29, wps=19817.6, ups=6.17, wpb=3212.1, bsz=333.4, num_updates=2000, lr=0.00025, gnorm=1.726, loss_scale=32, train_wall=16, gb_free=11.2, wall=26]2024-06-29 17:36:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  6.67it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.64it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 17:36:52 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 5.827 | nll_loss 4.359 | ppl 20.52 | wps 64224.9 | wpb 2381.8 | bsz 250 | num_updates 2044 | best_loss 5.827\n",
            "2024-06-29 17:36:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 2044 updates\n",
            "2024-06-29 17:36:52 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:37:00 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:37:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 11 @ 2044 updates, score 5.827) (writing took 12.138970441999845 seconds)\n",
            "2024-06-29 17:37:04 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2024-06-29 17:37:04 | INFO | train | epoch 011 | loss 5.352 | nll_loss 3.946 | ppl 15.41 | wps 14409.2 | ups 4.41 | wpb 3270.4 | bsz 344.1 | num_updates 2044 | lr 0.0002555 | gnorm 1.685 | loss_scale 32 | train_wall 30 | gb_free 11.4 | wall 45\n",
            "2024-06-29 17:37:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 012:   0% 0/186 [00:00<?, ?it/s]2024-06-29 17:37:04 | INFO | fairseq.trainer | begin training epoch 12\n",
            "2024-06-29 17:37:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 012:  99% 185/186 [00:30<00:00,  6.38it/s, loss=4.985, nll_loss=3.522, ppl=11.49, wps=20149.2, ups=6.21, wpb=3244.7, bsz=347.5, num_updates=2200, lr=0.000275, gnorm=1.535, loss_scale=32, train_wall=16, gb_free=11.4, wall=71]2024-06-29 17:37:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  50% 2/4 [00:00<00:00, 16.80it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 17:37:35 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 5.507 | nll_loss 3.992 | ppl 15.91 | wps 60138.4 | wpb 2381.8 | bsz 250 | num_updates 2230 | best_loss 5.507\n",
            "2024-06-29 17:37:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 2230 updates\n",
            "2024-06-29 17:37:35 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:37:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:37:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 12 @ 2230 updates, score 5.507) (writing took 9.729099419000022 seconds)\n",
            "2024-06-29 17:37:44 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2024-06-29 17:37:44 | INFO | train | epoch 012 | loss 5.021 | nll_loss 3.565 | ppl 11.84 | wps 15107.1 | ups 4.62 | wpb 3270.4 | bsz 344.1 | num_updates 2230 | lr 0.00027875 | gnorm 1.535 | loss_scale 32 | train_wall 29 | gb_free 11.4 | wall 85\n",
            "2024-06-29 17:37:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 013:   0% 0/186 [00:00<?, ?it/s]2024-06-29 17:37:45 | INFO | fairseq.trainer | begin training epoch 13\n",
            "2024-06-29 17:37:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 013:  99% 185/186 [00:30<00:00,  6.48it/s, loss=4.755, nll_loss=3.257, ppl=9.56, wps=20056.4, ups=6.17, wpb=3250.7, bsz=339.4, num_updates=2400, lr=0.0003, gnorm=1.451, loss_scale=32, train_wall=16, gb_free=11.3, wall=113]2024-06-29 17:38:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 013 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  50% 2/4 [00:00<00:00, 17.98it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 17:38:15 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 5.314 | nll_loss 3.734 | ppl 13.31 | wps 61032.4 | wpb 2381.8 | bsz 250 | num_updates 2416 | best_loss 5.314\n",
            "2024-06-29 17:38:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2416 updates\n",
            "2024-06-29 17:38:15 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:39:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:39:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 13 @ 2416 updates, score 5.314) (writing took 62.42190309400007 seconds)\n",
            "2024-06-29 17:39:18 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
            "2024-06-29 17:39:18 | INFO | train | epoch 013 | loss 4.741 | nll_loss 3.242 | ppl 9.46 | wps 6536.7 | ups 2 | wpb 3270.4 | bsz 344.1 | num_updates 2416 | lr 0.000302 | gnorm 1.478 | loss_scale 32 | train_wall 29 | gb_free 11.5 | wall 178\n",
            "2024-06-29 17:39:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 014:   0% 0/186 [00:00<?, ?it/s]2024-06-29 17:39:18 | INFO | fairseq.trainer | begin training epoch 14\n",
            "2024-06-29 17:39:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 014:  99% 185/186 [00:30<00:00,  6.27it/s, loss=4.487, nll_loss=2.947, ppl=7.71, wps=19919.2, ups=6.04, wpb=3297.6, bsz=350.4, num_updates=2600, lr=0.000325, gnorm=1.414, loss_scale=32, train_wall=16, gb_free=11.4, wall=209]2024-06-29 17:39:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 014 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  50% 2/4 [00:00<00:00, 12.37it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 17:39:49 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 5.068 | nll_loss 3.463 | ppl 11.02 | wps 43261.4 | wpb 2381.8 | bsz 250 | num_updates 2602 | best_loss 5.068\n",
            "2024-06-29 17:39:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2602 updates\n",
            "2024-06-29 17:39:49 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:40:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:41:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 14 @ 2602 updates, score 5.068) (writing took 76.95332590299995 seconds)\n",
            "2024-06-29 17:41:06 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
            "2024-06-29 17:41:06 | INFO | train | epoch 014 | loss 4.513 | nll_loss 2.979 | ppl 7.88 | wps 5632.2 | ups 1.72 | wpb 3270.4 | bsz 344.1 | num_updates 2602 | lr 0.00032525 | gnorm 1.433 | loss_scale 32 | train_wall 30 | gb_free 11.3 | wall 286\n",
            "2024-06-29 17:41:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 015:   0% 0/186 [00:00<?, ?it/s]2024-06-29 17:41:06 | INFO | fairseq.trainer | begin training epoch 15\n",
            "2024-06-29 17:41:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 015:  99% 185/186 [00:30<00:00,  6.35it/s, loss=4.294, nll_loss=2.727, ppl=6.62, wps=3449.9, ups=1.06, wpb=3240.9, bsz=340.7, num_updates=2700, lr=0.0003375, gnorm=1.322, loss_scale=32, train_wall=16, gb_free=11.3, wall=303]2024-06-29 17:41:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 015 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  50% 2/4 [00:00<00:00, 16.79it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 17:41:37 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 4.974 | nll_loss 3.353 | ppl 10.22 | wps 56494.7 | wpb 2381.8 | bsz 250 | num_updates 2788 | best_loss 4.974\n",
            "2024-06-29 17:41:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2788 updates\n",
            "2024-06-29 17:41:37 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:42:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:42:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 15 @ 2788 updates, score 4.974) (writing took 32.027514569999994 seconds)\n",
            "2024-06-29 17:42:09 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
            "2024-06-29 17:42:09 | INFO | train | epoch 015 | loss 4.292 | nll_loss 2.723 | ppl 6.6 | wps 9603 | ups 2.94 | wpb 3270.4 | bsz 344.1 | num_updates 2788 | lr 0.0003485 | gnorm 1.333 | loss_scale 32 | train_wall 30 | gb_free 11.1 | wall 350\n",
            "2024-06-29 17:42:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 016:   0% 0/186 [00:00<?, ?it/s]2024-06-29 17:42:09 | INFO | fairseq.trainer | begin training epoch 16\n",
            "2024-06-29 17:42:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 016:  99% 185/186 [00:30<00:00,  6.38it/s, loss=4.134, nll_loss=2.541, ppl=5.82, wps=19750.5, ups=6.01, wpb=3285, bsz=336.6, num_updates=2900, lr=0.0003625, gnorm=1.295, loss_scale=32, train_wall=16, gb_free=11.2, wall=368]2024-06-29 17:42:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 016 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  50% 2/4 [00:00<00:00, 18.45it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 17:42:40 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 4.855 | nll_loss 3.219 | ppl 9.31 | wps 61370.2 | wpb 2381.8 | bsz 250 | num_updates 2974 | best_loss 4.855\n",
            "2024-06-29 17:42:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 2974 updates\n",
            "2024-06-29 17:42:40 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:43:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:43:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 16 @ 2974 updates, score 4.855) (writing took 68.42540033299997 seconds)\n",
            "2024-06-29 17:43:49 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
            "2024-06-29 17:43:49 | INFO | train | epoch 016 | loss 4.097 | nll_loss 2.498 | ppl 5.65 | wps 6103.3 | ups 1.87 | wpb 3270.4 | bsz 344.1 | num_updates 2974 | lr 0.00037175 | gnorm 1.278 | loss_scale 32 | train_wall 30 | gb_free 11.5 | wall 449\n",
            "2024-06-29 17:43:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 017:   0% 0/186 [00:00<?, ?it/s]2024-06-29 17:43:49 | INFO | fairseq.trainer | begin training epoch 17\n",
            "2024-06-29 17:43:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 017:  99% 185/186 [00:30<00:00,  6.26it/s, loss=3.974, nll_loss=2.353, ppl=5.11, wps=19218.8, ups=5.9, wpb=3257.9, bsz=335.9, num_updates=3100, lr=0.0003875, gnorm=1.273, loss_scale=32, train_wall=16, gb_free=11.4, wall=471]2024-06-29 17:44:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 017 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  50% 2/4 [00:00<00:00, 18.11it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 17:44:20 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.777 | nll_loss 3.102 | ppl 8.59 | wps 61235.2 | wpb 2381.8 | bsz 250 | num_updates 3160 | best_loss 4.777\n",
            "2024-06-29 17:44:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 3160 updates\n",
            "2024-06-29 17:44:20 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:45:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:45:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 17 @ 3160 updates, score 4.777) (writing took 60.383079496999926 seconds)\n",
            "2024-06-29 17:45:20 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
            "2024-06-29 17:45:20 | INFO | train | epoch 017 | loss 3.952 | nll_loss 2.327 | ppl 5.02 | wps 6644.5 | ups 2.03 | wpb 3270.4 | bsz 344.1 | num_updates 3160 | lr 0.000395 | gnorm 1.243 | loss_scale 32 | train_wall 30 | gb_free 11.4 | wall 541\n",
            "2024-06-29 17:45:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 018:   0% 0/186 [00:00<?, ?it/s]2024-06-29 17:45:20 | INFO | fairseq.trainer | begin training epoch 18\n",
            "2024-06-29 17:45:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 018:  99% 185/186 [00:30<00:00,  6.09it/s, loss=3.819, nll_loss=2.171, ppl=4.5, wps=19852.8, ups=6.1, wpb=3256.7, bsz=342.4, num_updates=3300, lr=0.0004125, gnorm=1.173, loss_scale=32, train_wall=16, gb_free=11.6, wall=564]2024-06-29 17:45:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 018 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  9.06it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.84it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 17:45:51 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.704 | nll_loss 3.005 | ppl 8.03 | wps 54829.6 | wpb 2381.8 | bsz 250 | num_updates 3346 | best_loss 4.704\n",
            "2024-06-29 17:45:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 3346 updates\n",
            "2024-06-29 17:45:51 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:46:56 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:47:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 18 @ 3346 updates, score 4.704) (writing took 69.85636777799982 seconds)\n",
            "2024-06-29 17:47:01 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
            "2024-06-29 17:47:01 | INFO | train | epoch 018 | loss 3.822 | nll_loss 2.175 | ppl 4.52 | wps 6031 | ups 1.84 | wpb 3270.4 | bsz 344.1 | num_updates 3346 | lr 0.00041825 | gnorm 1.228 | loss_scale 32 | train_wall 30 | gb_free 11.1 | wall 642\n",
            "2024-06-29 17:47:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 019:   0% 0/186 [00:00<?, ?it/s]2024-06-29 17:47:01 | INFO | fairseq.trainer | begin training epoch 19\n",
            "2024-06-29 17:47:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 019:  99% 185/186 [00:30<00:00,  6.16it/s, loss=3.691, nll_loss=2.021, ppl=4.06, wps=20175.6, ups=6.04, wpb=3340.8, bsz=349.8, num_updates=3500, lr=0.0004375, gnorm=1.14, loss_scale=32, train_wall=16, gb_free=11.2, wall=667]2024-06-29 17:47:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 019 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  50% 2/4 [00:00<00:00, 18.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 17:47:32 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 4.685 | nll_loss 2.982 | ppl 7.9 | wps 61144.9 | wpb 2381.8 | bsz 250 | num_updates 3532 | best_loss 4.685\n",
            "2024-06-29 17:47:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3532 updates\n",
            "2024-06-29 17:47:32 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:47:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:48:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 19 @ 3532 updates, score 4.685) (writing took 36.66228604099979 seconds)\n",
            "2024-06-29 17:48:09 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
            "2024-06-29 17:48:09 | INFO | train | epoch 019 | loss 3.705 | nll_loss 2.037 | ppl 4.1 | wps 8959.1 | ups 2.74 | wpb 3270.4 | bsz 344.1 | num_updates 3532 | lr 0.0004415 | gnorm 1.198 | loss_scale 32 | train_wall 30 | gb_free 11.5 | wall 710\n",
            "2024-06-29 17:48:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 020:   0% 0/186 [00:00<?, ?it/s]2024-06-29 17:48:09 | INFO | fairseq.trainer | begin training epoch 20\n",
            "2024-06-29 17:48:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 020:  99% 185/186 [00:30<00:00,  6.35it/s, loss=3.606, nll_loss=1.918, ppl=3.78, wps=19510.7, ups=5.97, wpb=3265.8, bsz=353, num_updates=3700, lr=0.0004625, gnorm=1.141, loss_scale=32, train_wall=16, gb_free=11.6, wall=738]2024-06-29 17:48:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 020 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  50% 2/4 [00:00<00:00, 16.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 17:48:40 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 4.668 | nll_loss 2.949 | ppl 7.72 | wps 59199.5 | wpb 2381.8 | bsz 250 | num_updates 3718 | best_loss 4.668\n",
            "2024-06-29 17:48:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3718 updates\n",
            "2024-06-29 17:48:40 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:49:13 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:49:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 20 @ 3718 updates, score 4.668) (writing took 37.69838774799973 seconds)\n",
            "2024-06-29 17:49:18 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
            "2024-06-29 17:49:18 | INFO | train | epoch 020 | loss 3.592 | nll_loss 1.905 | ppl 3.75 | wps 8837.5 | ups 2.7 | wpb 3270.4 | bsz 344.1 | num_updates 3718 | lr 0.00046475 | gnorm 1.159 | loss_scale 32 | train_wall 30 | gb_free 11.1 | wall 778\n",
            "2024-06-29 17:49:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 021:   0% 0/186 [00:00<?, ?it/s]2024-06-29 17:49:18 | INFO | fairseq.trainer | begin training epoch 21\n",
            "2024-06-29 17:49:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 021:  99% 185/186 [00:30<00:00,  6.51it/s, loss=3.513, nll_loss=1.809, ppl=3.5, wps=19720.5, ups=6.04, wpb=3266.6, bsz=343.8, num_updates=3900, lr=0.0004875, gnorm=1.093, loss_scale=32, train_wall=16, gb_free=11.6, wall=809]2024-06-29 17:49:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 021 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  50% 2/4 [00:00<00:00, 14.18it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 17:49:49 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 4.618 | nll_loss 2.876 | ppl 7.34 | wps 44638.4 | wpb 2381.8 | bsz 250 | num_updates 3904 | best_loss 4.618\n",
            "2024-06-29 17:49:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 3904 updates\n",
            "2024-06-29 17:49:49 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:50:02 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:50:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 21 @ 3904 updates, score 4.618) (writing took 29.96423735799999 seconds)\n",
            "2024-06-29 17:50:19 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
            "2024-06-29 17:50:19 | INFO | train | epoch 021 | loss 3.484 | nll_loss 1.779 | ppl 3.43 | wps 9959.3 | ups 3.05 | wpb 3270.4 | bsz 344.1 | num_updates 3904 | lr 0.000488 | gnorm 1.107 | loss_scale 32 | train_wall 30 | gb_free 11.4 | wall 839\n",
            "2024-06-29 17:50:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 022:   0% 0/186 [00:00<?, ?it/s]2024-06-29 17:50:19 | INFO | fairseq.trainer | begin training epoch 22\n",
            "2024-06-29 17:50:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 022:  99% 185/186 [00:30<00:00,  6.38it/s, loss=3.378, nll_loss=1.655, ppl=3.15, wps=6999.4, ups=2.15, wpb=3261, bsz=341.7, num_updates=4000, lr=0.0005, gnorm=1.115, loss_scale=32, train_wall=16, gb_free=11.1, wall=855]2024-06-29 17:50:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 022 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  50% 2/4 [00:00<00:00, 17.34it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 17:50:50 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 4.557 | nll_loss 2.837 | ppl 7.14 | wps 62927 | wpb 2381.8 | bsz 250 | num_updates 4090 | best_loss 4.557\n",
            "2024-06-29 17:50:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 4090 updates\n",
            "2024-06-29 17:50:50 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:51:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:51:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 22 @ 4090 updates, score 4.557) (writing took 36.95644358399977 seconds)\n",
            "2024-06-29 17:51:27 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
            "2024-06-29 17:51:27 | INFO | train | epoch 022 | loss 3.407 | nll_loss 1.686 | ppl 3.22 | wps 8976 | ups 2.74 | wpb 3270.4 | bsz 344.1 | num_updates 4090 | lr 0.000494468 | gnorm 1.107 | loss_scale 32 | train_wall 30 | gb_free 11.4 | wall 907\n",
            "2024-06-29 17:51:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 023:   0% 0/186 [00:00<?, ?it/s]2024-06-29 17:51:27 | INFO | fairseq.trainer | begin training epoch 23\n",
            "2024-06-29 17:51:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 023:  99% 185/186 [00:30<00:00,  6.31it/s, loss=3.262, nll_loss=1.519, ppl=2.87, wps=19867.4, ups=5.94, wpb=3344.4, bsz=351.3, num_updates=4200, lr=0.00048795, gnorm=1.074, loss_scale=32, train_wall=16, gb_free=11.1, wall=926]2024-06-29 17:51:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 023 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  25% 1/4 [00:00<00:00,  9.00it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset: 100% 4/4 [00:00<00:00, 17.94it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 17:51:58 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 4.626 | nll_loss 2.88 | ppl 7.36 | wps 50003.9 | wpb 2381.8 | bsz 250 | num_updates 4276 | best_loss 4.557\n",
            "2024-06-29 17:51:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 4276 updates\n",
            "2024-06-29 17:51:58 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 17:52:52 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 17:52:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_last.pt (epoch 23 @ 4276 updates, score 4.626) (writing took 54.52271000200017 seconds)\n",
            "2024-06-29 17:52:52 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
            "2024-06-29 17:52:52 | INFO | train | epoch 023 | loss 3.306 | nll_loss 1.569 | ppl 2.97 | wps 7096.2 | ups 2.17 | wpb 3270.4 | bsz 344.1 | num_updates 4276 | lr 0.000483594 | gnorm 1.104 | loss_scale 32 | train_wall 30 | gb_free 11.2 | wall 993\n",
            "2024-06-29 17:52:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 024:   0% 0/186 [00:00<?, ?it/s]2024-06-29 17:52:52 | INFO | fairseq.trainer | begin training epoch 24\n",
            "2024-06-29 17:52:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 024:  99% 185/186 [00:30<00:00,  6.63it/s, loss=3.144, nll_loss=1.383, ppl=2.61, wps=19876.3, ups=5.98, wpb=3325.9, bsz=352.3, num_updates=4400, lr=0.000476731, gnorm=0.975, loss_scale=32, train_wall=16, gb_free=11.4, wall=1014]2024-06-29 17:53:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 024 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  50% 2/4 [00:00<00:00, 15.98it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 17:53:23 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 4.589 | nll_loss 2.855 | ppl 7.23 | wps 55627.8 | wpb 2381.8 | bsz 250 | num_updates 4462 | best_loss 4.557\n",
            "2024-06-29 17:53:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 4462 updates\n",
            "2024-06-29 17:53:23 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 17:53:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 17:53:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_last.pt (epoch 24 @ 4462 updates, score 4.589) (writing took 16.278760649000105 seconds)\n",
            "2024-06-29 17:53:40 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
            "2024-06-29 17:53:40 | INFO | train | epoch 024 | loss 3.184 | nll_loss 1.428 | ppl 2.69 | wps 12823.9 | ups 3.92 | wpb 3270.4 | bsz 344.1 | num_updates 4462 | lr 0.000473408 | gnorm 1.009 | loss_scale 32 | train_wall 30 | gb_free 11.6 | wall 1040\n",
            "2024-06-29 17:53:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 025:   0% 0/186 [00:00<?, ?it/s]2024-06-29 17:53:40 | INFO | fairseq.trainer | begin training epoch 25\n",
            "2024-06-29 17:53:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 025:  99% 185/186 [00:30<00:00,  6.56it/s, loss=3.069, nll_loss=1.295, ppl=2.45, wps=19591, ups=6.03, wpb=3250.4, bsz=357.4, num_updates=4600, lr=0.000466252, gnorm=0.985, loss_scale=32, train_wall=16, gb_free=11.7, wall=1063]2024-06-29 17:54:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 025 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  50% 2/4 [00:00<00:00, 18.26it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 17:54:11 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 4.628 | nll_loss 2.908 | ppl 7.5 | wps 59208.2 | wpb 2381.8 | bsz 250 | num_updates 4648 | best_loss 4.557\n",
            "2024-06-29 17:54:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 4648 updates\n",
            "2024-06-29 17:54:11 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 17:55:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 17:55:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_last.pt (epoch 25 @ 4648 updates, score 4.628) (writing took 52.324197553999966 seconds)\n",
            "2024-06-29 17:55:03 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
            "2024-06-29 17:55:03 | INFO | train | epoch 025 | loss 3.089 | nll_loss 1.318 | ppl 2.49 | wps 7285 | ups 2.23 | wpb 3270.4 | bsz 344.1 | num_updates 4648 | lr 0.000463839 | gnorm 0.989 | loss_scale 32 | train_wall 30 | gb_free 11.1 | wall 1124\n",
            "2024-06-29 17:55:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 026:   0% 0/186 [00:00<?, ?it/s]2024-06-29 17:55:03 | INFO | fairseq.trainer | begin training epoch 26\n",
            "2024-06-29 17:55:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 026:  99% 185/186 [00:30<00:00,  6.21it/s, loss=2.984, nll_loss=1.197, ppl=2.29, wps=19882.4, ups=6.16, wpb=3226.7, bsz=344, num_updates=4800, lr=0.000456435, gnorm=0.947, loss_scale=32, train_wall=16, gb_free=11.5, wall=1149]2024-06-29 17:55:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 026 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  50% 2/4 [00:00<00:00, 11.75it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 17:55:34 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 4.567 | nll_loss 2.846 | ppl 7.19 | wps 39302.8 | wpb 2381.8 | bsz 250 | num_updates 4834 | best_loss 4.557\n",
            "2024-06-29 17:55:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 4834 updates\n",
            "2024-06-29 17:55:34 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 17:55:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 17:55:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_last.pt (epoch 26 @ 4834 updates, score 4.567) (writing took 15.147015987000032 seconds)\n",
            "2024-06-29 17:55:50 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
            "2024-06-29 17:55:50 | INFO | train | epoch 026 | loss 2.992 | nll_loss 1.206 | ppl 2.31 | wps 13124.3 | ups 4.01 | wpb 3270.4 | bsz 344.1 | num_updates 4834 | lr 0.000454827 | gnorm 0.958 | loss_scale 32 | train_wall 30 | gb_free 11.1 | wall 1170\n",
            "2024-06-29 17:55:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 027:   0% 0/186 [00:00<?, ?it/s]2024-06-29 17:55:50 | INFO | fairseq.trainer | begin training epoch 27\n",
            "2024-06-29 17:55:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 027:  99% 185/186 [00:30<00:00,  6.35it/s, loss=2.956, nll_loss=1.165, ppl=2.24, wps=19850.5, ups=6.12, wpb=3245.5, bsz=341.3, num_updates=5000, lr=0.000447214, gnorm=0.988, loss_scale=32, train_wall=16, gb_free=11.5, wall=1198]2024-06-29 17:56:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 027 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  50% 2/4 [00:00<00:00, 16.94it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 17:56:20 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 4.555 | nll_loss 2.84 | ppl 7.16 | wps 58382.8 | wpb 2381.8 | bsz 250 | num_updates 5020 | best_loss 4.555\n",
            "2024-06-29 17:56:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 5020 updates\n",
            "2024-06-29 17:56:20 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:56:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 17:56:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 27 @ 5020 updates, score 4.555) (writing took 35.52422993399978 seconds)\n",
            "2024-06-29 17:56:56 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
            "2024-06-29 17:56:56 | INFO | train | epoch 027 | loss 2.918 | nll_loss 1.121 | ppl 2.17 | wps 9162.4 | ups 2.8 | wpb 3270.4 | bsz 344.1 | num_updates 5020 | lr 0.000446322 | gnorm 0.948 | loss_scale 32 | train_wall 30 | gb_free 11.2 | wall 1237\n",
            "2024-06-29 17:56:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 028:   0% 0/186 [00:00<?, ?it/s]2024-06-29 17:56:56 | INFO | fairseq.trainer | begin training epoch 28\n",
            "2024-06-29 17:56:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 028:  99% 185/186 [00:30<00:00,  6.39it/s, loss=2.867, nll_loss=1.062, ppl=2.09, wps=20041.1, ups=6.07, wpb=3300.6, bsz=348.6, num_updates=5200, lr=0.000438529, gnorm=0.925, loss_scale=32, train_wall=16, gb_free=11.5, wall=1266]2024-06-29 17:57:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 028 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  50% 2/4 [00:00<00:00, 15.74it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 17:57:27 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 4.557 | nll_loss 2.838 | ppl 7.15 | wps 63707.9 | wpb 2381.8 | bsz 250 | num_updates 5206 | best_loss 4.555\n",
            "2024-06-29 17:57:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 5206 updates\n",
            "2024-06-29 17:57:27 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 17:58:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 17:58:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_last.pt (epoch 28 @ 5206 updates, score 4.557) (writing took 71.12065560700012 seconds)\n",
            "2024-06-29 17:58:38 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n",
            "2024-06-29 17:58:38 | INFO | train | epoch 028 | loss 2.836 | nll_loss 1.026 | ppl 2.04 | wps 5959.4 | ups 1.82 | wpb 3270.4 | bsz 344.1 | num_updates 5206 | lr 0.000438276 | gnorm 0.904 | loss_scale 32 | train_wall 30 | gb_free 11.4 | wall 1339\n",
            "2024-06-29 17:58:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 029:   0% 0/186 [00:00<?, ?it/s]2024-06-29 17:58:38 | INFO | fairseq.trainer | begin training epoch 29\n",
            "2024-06-29 17:58:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 029:  99% 185/186 [00:30<00:00,  6.26it/s, loss=2.756, nll_loss=0.934, ppl=1.91, wps=3734.2, ups=1.14, wpb=3284.2, bsz=338.4, num_updates=5300, lr=0.000434372, gnorm=0.906, loss_scale=32, train_wall=16, gb_free=11.4, wall=1354]2024-06-29 17:59:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 029 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  50% 2/4 [00:00<00:00, 11.95it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 17:59:09 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 4.563 | nll_loss 2.825 | ppl 7.09 | wps 43577.4 | wpb 2381.8 | bsz 250 | num_updates 5392 | best_loss 4.555\n",
            "2024-06-29 17:59:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 5392 updates\n",
            "2024-06-29 17:59:09 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 17:59:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 17:59:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_last.pt (epoch 29 @ 5392 updates, score 4.563) (writing took 9.289887118000024 seconds)\n",
            "2024-06-29 17:59:19 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n",
            "2024-06-29 17:59:19 | INFO | train | epoch 029 | loss 2.778 | nll_loss 0.96 | ppl 1.95 | wps 14946.4 | ups 4.57 | wpb 3270.4 | bsz 344.1 | num_updates 5392 | lr 0.000430651 | gnorm 0.909 | loss_scale 32 | train_wall 30 | gb_free 11.4 | wall 1379\n",
            "2024-06-29 17:59:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 030:   0% 0/186 [00:00<?, ?it/s]2024-06-29 17:59:19 | INFO | fairseq.trainer | begin training epoch 30\n",
            "2024-06-29 17:59:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 030:  99% 185/186 [00:30<00:00,  6.21it/s, loss=2.669, nll_loss=0.836, ppl=1.78, wps=19733.6, ups=6.06, wpb=3254.9, bsz=355.7, num_updates=5500, lr=0.000426401, gnorm=0.824, loss_scale=32, train_wall=16, gb_free=11.1, wall=1397]2024-06-29 17:59:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 030 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  50% 2/4 [00:00<00:00, 14.82it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 17:59:50 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 4.552 | nll_loss 2.824 | ppl 7.08 | wps 53077.8 | wpb 2381.8 | bsz 250 | num_updates 5578 | best_loss 4.552\n",
            "2024-06-29 17:59:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 5578 updates\n",
            "2024-06-29 17:59:50 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 18:00:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_best.pt\n",
            "2024-06-29 18:00:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_best.pt (epoch 30 @ 5578 updates, score 4.552) (writing took 52.58147890800001 seconds)\n",
            "2024-06-29 18:00:42 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n",
            "2024-06-29 18:00:42 | INFO | train | epoch 030 | loss 2.714 | nll_loss 0.888 | ppl 1.85 | wps 7263.4 | ups 2.22 | wpb 3270.4 | bsz 344.1 | num_updates 5578 | lr 0.00042341 | gnorm 0.865 | loss_scale 32 | train_wall 30 | gb_free 11.2 | wall 1463\n",
            "2024-06-29 18:00:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 031:   0% 0/186 [00:00<?, ?it/s]2024-06-29 18:00:42 | INFO | fairseq.trainer | begin training epoch 31\n",
            "2024-06-29 18:00:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 031:  99% 185/186 [00:30<00:00,  6.14it/s, loss=2.631, nll_loss=0.792, ppl=1.73, wps=19913.9, ups=6.09, wpb=3267.9, bsz=345.3, num_updates=5700, lr=0.000418854, gnorm=0.807, loss_scale=32, train_wall=16, gb_free=11.4, wall=1483]2024-06-29 18:01:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 031 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  50% 2/4 [00:00<00:00, 12.76it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 18:01:14 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 4.56 | nll_loss 2.846 | ppl 7.19 | wps 45255.2 | wpb 2381.8 | bsz 250 | num_updates 5764 | best_loss 4.552\n",
            "2024-06-29 18:01:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 5764 updates\n",
            "2024-06-29 18:01:14 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 18:01:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 18:01:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_last.pt (epoch 31 @ 5764 updates, score 4.56) (writing took 25.815877133000413 seconds)\n",
            "2024-06-29 18:01:40 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n",
            "2024-06-29 18:01:40 | INFO | train | epoch 031 | loss 2.655 | nll_loss 0.819 | ppl 1.76 | wps 10646.4 | ups 3.26 | wpb 3270.4 | bsz 344.1 | num_updates 5764 | lr 0.000416522 | gnorm 0.842 | loss_scale 32 | train_wall 30 | gb_free 11.2 | wall 1520\n",
            "2024-06-29 18:01:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 032:   0% 0/186 [00:00<?, ?it/s]2024-06-29 18:01:40 | INFO | fairseq.trainer | begin training epoch 32\n",
            "2024-06-29 18:01:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 032:  99% 185/186 [00:30<00:00,  6.18it/s, loss=2.606, nll_loss=0.765, ppl=1.7, wps=20033, ups=6.11, wpb=3277.9, bsz=345.1, num_updates=5900, lr=0.000411693, gnorm=0.805, loss_scale=32, train_wall=16, gb_free=11.1, wall=1543]2024-06-29 18:02:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 032 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  50% 2/4 [00:00<00:00, 12.77it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 18:02:11 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 4.581 | nll_loss 2.876 | ppl 7.34 | wps 43035 | wpb 2381.8 | bsz 250 | num_updates 5950 | best_loss 4.552\n",
            "2024-06-29 18:02:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 5950 updates\n",
            "2024-06-29 18:02:11 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 18:02:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 18:02:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_last.pt (epoch 32 @ 5950 updates, score 4.581) (writing took 37.530208996000056 seconds)\n",
            "2024-06-29 18:02:48 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n",
            "2024-06-29 18:02:48 | INFO | train | epoch 032 | loss 2.608 | nll_loss 0.767 | ppl 1.7 | wps 8848.9 | ups 2.71 | wpb 3270.4 | bsz 344.1 | num_updates 5950 | lr 0.00040996 | gnorm 0.814 | loss_scale 32 | train_wall 30 | gb_free 11.1 | wall 1589\n",
            "2024-06-29 18:02:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 033:   0% 0/186 [00:00<?, ?it/s]2024-06-29 18:02:48 | INFO | fairseq.trainer | begin training epoch 33\n",
            "2024-06-29 18:02:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 033:  99% 185/186 [00:30<00:00,  6.44it/s, loss=2.569, nll_loss=0.724, ppl=1.65, wps=19691.5, ups=5.94, wpb=3314, bsz=340.2, num_updates=6100, lr=0.000404888, gnorm=0.8, loss_scale=32, train_wall=16, gb_free=11.2, wall=1614]2024-06-29 18:03:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 033 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  50% 2/4 [00:00<00:00, 18.30it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 18:03:19 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 4.585 | nll_loss 2.868 | ppl 7.3 | wps 61770.8 | wpb 2381.8 | bsz 250 | num_updates 6136 | best_loss 4.552\n",
            "2024-06-29 18:03:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 6136 updates\n",
            "2024-06-29 18:03:19 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 18:03:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 18:03:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_last.pt (epoch 33 @ 6136 updates, score 4.585) (writing took 20.357565652000176 seconds)\n",
            "2024-06-29 18:03:40 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n",
            "2024-06-29 18:03:40 | INFO | train | epoch 033 | loss 2.567 | nll_loss 0.721 | ppl 1.65 | wps 11811.9 | ups 3.61 | wpb 3270.4 | bsz 344.1 | num_updates 6136 | lr 0.000403699 | gnorm 0.807 | loss_scale 32 | train_wall 30 | gb_free 11.1 | wall 1641\n",
            "2024-06-29 18:03:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 034:   0% 0/186 [00:00<?, ?it/s]2024-06-29 18:03:40 | INFO | fairseq.trainer | begin training epoch 34\n",
            "2024-06-29 18:03:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 034:  99% 185/186 [00:30<00:00,  6.34it/s, loss=2.543, nll_loss=0.697, ppl=1.62, wps=19354.2, ups=5.95, wpb=3250.9, bsz=342.3, num_updates=6300, lr=0.00039841, gnorm=0.81, loss_scale=32, train_wall=16, gb_free=11.2, wall=1668]2024-06-29 18:04:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 034 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  50% 2/4 [00:00<00:00, 16.04it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 18:04:11 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 4.598 | nll_loss 2.889 | ppl 7.41 | wps 56960.6 | wpb 2381.8 | bsz 250 | num_updates 6322 | best_loss 4.552\n",
            "2024-06-29 18:04:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 6322 updates\n",
            "2024-06-29 18:04:11 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 18:04:25 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 18:04:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_last.pt (epoch 34 @ 6322 updates, score 4.598) (writing took 14.214868658999876 seconds)\n",
            "2024-06-29 18:04:25 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n",
            "2024-06-29 18:04:25 | INFO | train | epoch 034 | loss 2.528 | nll_loss 0.678 | ppl 1.6 | wps 13345 | ups 4.08 | wpb 3270.4 | bsz 344.1 | num_updates 6322 | lr 0.000397716 | gnorm 0.789 | loss_scale 32 | train_wall 30 | gb_free 11.3 | wall 1686\n",
            "2024-06-29 18:04:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 186\n",
            "epoch 035:   0% 0/186 [00:00<?, ?it/s]2024-06-29 18:04:25 | INFO | fairseq.trainer | begin training epoch 35\n",
            "2024-06-29 18:04:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 035:  99% 185/186 [00:30<00:00,  6.35it/s, loss=2.515, nll_loss=0.667, ppl=1.59, wps=19883.7, ups=6.09, wpb=3263.4, bsz=342.8, num_updates=6500, lr=0.000392232, gnorm=0.792, loss_scale=32, train_wall=16, gb_free=11.6, wall=1716]2024-06-29 18:04:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 035 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  50% 2/4 [00:00<00:00, 16.16it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-06-29 18:04:56 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 4.6 | nll_loss 2.908 | ppl 7.5 | wps 54906.9 | wpb 2381.8 | bsz 250 | num_updates 6508 | best_loss 4.552\n",
            "2024-06-29 18:04:56 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 5 runs\n",
            "2024-06-29 18:04:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 6508 updates\n",
            "2024-06-29 18:04:57 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 18:05:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/transformer/checkpoint_last.pt\n",
            "2024-06-29 18:05:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer/checkpoint_last.pt (epoch 35 @ 6508 updates, score 4.6) (writing took 10.58300538100002 seconds)\n",
            "2024-06-29 18:05:07 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
            "2024-06-29 18:05:07 | INFO | train | epoch 035 | loss 2.492 | nll_loss 0.639 | ppl 1.56 | wps 14593.8 | ups 4.46 | wpb 3270.4 | bsz 344.1 | num_updates 6508 | lr 0.000391991 | gnorm 0.77 | loss_scale 32 | train_wall 30 | gb_free 11.6 | wall 1728\n",
            "2024-06-29 18:05:07 | INFO | fairseq_cli.train | done training in 1726.3 seconds\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 fairseq-train \\\n",
        "  data-bin/en-gu \\\n",
        "  --arch transformer \\\n",
        "  --encoder-layers 6 --decoder-layers 6 \\\n",
        "  --encoder-attention-heads 8 --decoder-attention-heads 8 \\\n",
        "  --encoder-embed-dim 512 --decoder-embed-dim 512 \\\n",
        "  --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
        "  --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
        "  --dropout 0.3 --weight-decay 0.0001 \\\n",
        "  --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "  --max-tokens 4096 \\\n",
        "  --save-dir checkpoints/transformer \\\n",
        "  --max-epoch 100 \\\n",
        "  --patience 5 \\\n",
        "  --no-epoch-checkpoints \\\n",
        "  --fp16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFUYH2T_0kGz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91cea964-f859-4f09-f757-0010be40850e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-29 17:26:00.829614: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-29 17:26:00.829675: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-29 17:26:00.831556: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-29 17:26:00.842076: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-29 17:26:02.519534: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-06-29 17:26:05 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2024-06-29 17:26:07 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/transformer/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 12000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin/en-gu', 'source_lang': 'en', 'target_lang': 'gu', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-06-29 17:26:07 | INFO | fairseq.tasks.translation | [en] dictionary: 17312 types\n",
            "2024-06-29 17:26:07 | INFO | fairseq.tasks.translation | [gu] dictionary: 25224 types\n",
            "2024-06-29 17:26:07 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/transformer/checkpoint_best.pt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-generate\", line 8, in <module>\n",
            "    sys.exit(cli_main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/generate.py\", line 413, in cli_main\n",
            "    main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/generate.py\", line 50, in main\n",
            "    return _main(cfg, sys.stdout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/generate.py\", line 106, in _main\n",
            "    task.load_dataset(cfg.dataset.gen_subset, task_cfg=saved_cfg.task)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/translation.py\", line 338, in load_dataset\n",
            "    self.datasets[split] = load_langpair_dataset(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/translation.py\", line 81, in load_langpair_dataset\n",
            "    raise FileNotFoundError(\n",
            "FileNotFoundError: Dataset not found: test (data-bin/en-gu)\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 fairseq-generate data-bin/en-gu \\\n",
        "  --path checkpoints/transformer/checkpoint_best.pt \\\n",
        "  --beam 5 --source-lang en --target-lang gu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-interactive data-bin/en-gu \\\n",
        "  --input test.en \\\n",
        "  --path checkpoints/transformer/checkpoint_best.pt \\\n",
        "  --beam 5 \\\n",
        "  --source-lang en \\\n",
        "  --target-lang gu \\\n",
        "  --tokenizer moses \\\n",
        "  --bpe subword_nmt \\\n",
        "  --bpe-codes path/to/bpe/codes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKOVJQsyCEKu",
        "outputId": "6addd1b2-9d7f-4957-d998-74568c9f767c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-29 18:13:40.333921: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-06-29 18:13:40.333979: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-06-29 18:13:40.335431: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-06-29 18:13:40.342729: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-06-29 18:13:41.433301: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-06-29 18:13:44 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2024-06-29 18:13:46 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/transformer/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': 'test.en'}, 'model': None, 'task': {'_name': 'translation', 'data': 'data-bin/en-gu', 'source_lang': 'en', 'target_lang': 'gu', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': {'_name': 'subword_nmt', 'bpe_codes': 'path/to/bpe/codes', 'bpe_separator': '@@'}, 'tokenizer': {'_name': 'moses', 'source_lang': 'en', 'target_lang': 'gu', 'moses_no_dash_splits': False, 'moses_no_escape': False}, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2024-06-29 18:13:46 | INFO | fairseq.tasks.translation | [en] dictionary: 17312 types\n",
            "2024-06-29 18:13:46 | INFO | fairseq.tasks.translation | [gu] dictionary: 25224 types\n",
            "2024-06-29 18:13:46 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/transformer/checkpoint_best.pt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-interactive\", line 8, in <module>\n",
            "    sys.exit(cli_main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/interactive.py\", line 313, in cli_main\n",
            "    distributed_utils.call_main(convert_namespace_to_omegaconf(args), main)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/distributed/utils.py\", line 369, in call_main\n",
            "    main(cfg, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/interactive.py\", line 173, in main\n",
            "    bpe = task.build_bpe(cfg.bpe)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 629, in build_bpe\n",
            "    return encoders.build_bpe(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/registry.py\", line 61, in build_x\n",
            "    return builder(cfg, *extra_args, **extra_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/data/encoders/subword_nmt_bpe.py\", line 24, in __init__\n",
            "    codes = file_utils.cached_path(cfg.bpe_codes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/file_utils.py\", line 180, in cached_path\n",
            "    raise EnvironmentError(\"file {} not found\".format(url_or_filename))\n",
            "OSError: file path/to/bpe/codes not found\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}